### Add from top:

22 June 6:
I updated fish so that they could start with a correct prior. 
This is sort of the default assumption, so let's see how the winner effect plays
when their estimate is fixed, correct, and gets boosted by win/lose.


22 June 3b:
Spent a long time working on my likelihood function for naive stuff. But things are working again. 
I think the big insight here is that without an accurate self assessment, you're going to be 
writing checks you can't cash, which, depending on how risky that is, could be bad. 

So Measuring fight level in excess of fish size might be a nice one, since that will happen a lot with
all the naive metrics, but doesn't happen much with bayes self assessment. Obviously, when you bring
opponent assessment into the mix, you can be very precise about the amount of effort you put in, 
but maybe self assessment is enough. 

22 June 3:
It occurred to me that maybe the initial prior is good enough that with opponent assessment, 
you will get consistent, accurate hierarchies regardless of the winner effect. 
To test this, I made a 'no_update' function, which just keeps the prior constant. 
Results:
accuracy drops to .82, which is low, but not as low as you might expect...Here the fight's are 
largely determined by size. Obviously if that changes, accuracy will drop as well. 

Frustratingly, bayes still doesn't perform better with pure self assessment. What happens if we make 
effort more influential? The point here also is that I'm measuring the accuracy of fights, not the 
accuracy of assessments. It may be that every fish thinks they're the biggest, even if they lose. 
While this WE is "accurate" in the sense that you don't get wrong hierarchies, there's a lot of wasted 
effort. And if there is a cost to effort, it's going to be bad. So I should be measuring that also...

The fact that bayes' isn't doing anything for me though makes me think there is something wrong...
there were some more bugs, and I started measuring effort and number of upsets, which are a nicer metric. 
Accuracy is okay...but on average, even when things are very bad, if size factors in, the hierarchy will
end up being roughly accurate. You need pretty strong winner effects, driven by initially wild estimates, 
with a lack of good opponent assessment (I should figure out those exact conditions where you get 
confident, stable, incorrect hierarchies, which are, imho, the coolest.) 

In any case, bayes opponent assessment looks like what I would expect, but bayes self assessment 
is pretty poor. They're just not figuring things out very well. I suspect my likelihood function is off. 
Narrator: Likelihood function was indeed off.

22 June 2:
Found a big bug in how effort was working, so everything will need to be redone. I expect this will happen a couple more times...
Still, no harm done yet. 
-I have this method for measuring 'estimate', but effort is nicer to measure. I wrote some code for that. 
 This however brings up the fact that how effort is decided is such a huge part of this. 

It's not just OA vs SA, how much effort should you give? Should you give max effort if you're guaranteed a win? 
Or should you give the minimum effort it requires to win? 
Seeing the biggest fish always giving max effort really raises some question as to the logic of that strategy
I guess I should think of this more as the max bid, but you only have to give as much as the opponent did.

With the effort fixed, I check accuracy. In order, most to least accurate: 
Decay, Fixed, Bayes, (all essentially equal) and then 10% back, Hock-Huber (which is basically ELO)
...It's a bit of a bummer that the stupid approaches are equally (if not more) accurate than bayesian updating. I'll need to think about that a bit. 

22 June 1: 
I think everything works now...some tests would probably help, but I'm going to set up the code to run some tests.
Hopefully any bugs will pop up as I go, if not I'll do some careful testing afterwards. 
Still need to add a little more though: 
-Calculating the longevity of the winner effect (the winner effect strength function is there, just iterate it)
-I feel like stability is still a little sus, but I might not use it that much anyway
Each test might as well be a custom script from here on out. 

I made a little demo of duration and strength being a function of information. I'll need to actually 
write a way to quantify that, so that I can then compare it to other winner-effect models, but we're getting there. 

22 May 31:
I've started working on the more naive winner effect approaches:
- A clipped, decaying boost. Several parameters here, but for now, after fight +/- .1 to effort (clipped to 0,1)
- Non decaying boost (+/- .1 to effort, clipped to (0,1), doesn't decay), just set self.decay = 1
I've also added a tank.function to calculate the strenght of the winner effect for each fish.

I need to clean up the update_prior methods, right now I use one for opponent assessment, 
and a differently structured one for self assessment. Would be much cleaner if they were consistent

22 May 27b:
So, things got too messy with the below approach, so to simplify self assessment, I assumed
that you only know your own effort, and you assume every fish you meet is average size and effort.
This actually seems to work pretty well. With pure SA, you get nice, correct hierarchies (assuming size 
matters somewhat). The function is set up so that you *could* use additional info if you wanted to, 
although I haven't tested that yet. 

22 May 27:
I got back into the likelihood function, and boy is that a mess. 
So it's probably not very reasonable to say that fish know exactly the size of their opponent, especially
if we're doing a Self Assessment model. So in that case, what do fish know? 
    My assumptions are that they know Own Effort, and the Opponent Wager. 
So that's pretty good, and then you can just infer your size based on those two things, EXCEPT!! The 
size variable that goes into determining the fight is relative size. So in order to do a likelihood function
of relative size, you need to know something about the size of the opponent (or the effort). 

I think the most reasonable assumption to make there is that in pure self assessment, you assume average
effort by the other fish. So I set effort to .5, and then infer the size of the opponent from that and then 
calculate a likelihood function for relative size in that way. This is basically a transformation of my size
to relative size for that fight. Obviously, if you know the opponent's size, this is much easier (and I should
probably update that as well eventually...). 

So in summary, under SA, fish base their likelihood functions from their known effort, the assumed effort of the opponent, and the experienced wager from the opponent.

Even this is a little weird, like you only really know that the opponent wagered as much as or more than you...
but this feels fine for now. Definitely something to think about. 

I think in the long run, we'll have these general predictions, and then we'll see what it takes to fit the data, and then we'll ask if such a model is biologically plausible. 

22 May 26:
Did a lot of work to get Hock-Huber approach working. I made a slight modification to the way they calculate probability so that it matches
the more empirically driven idea of the outcome being decided by the percent difference, with marginal effort having a bigger impact when 
you're close to the opponent's investment. 

I also brought it into my wager function, so that I could play with the amount of luck involved, as well as add a size component (by default, 
Hock assumed l=0.5, which is a linear relationship between relative effort and probability of winning)

Surprisingly, although the stability of the hock estimate swings widely, as I would have guessed, the stability of fights is fairly comparable. 
The caveat here is I still don't entirely trust my stability metric...so I should check on that next. 
On top of that, I should probably calculate the strength of the winner effect: i.e. the increased probability of winning following a win. 


22 May 25:
Ran all the simulations, things are looking quite good. Ended with running Hock approach
One thing to keep in mind here is that "stability" measures fight performance, while "estimate" measures internal state. 
So when I'm plotting the est_history, I'm plotting their internal states, while stability is based on their wins/losses. 

In any case, it does seem like Hock is a lot less stable, but that could be in part just because of the inherent luck in the approach. 
The next thing to do in order for a fair comparison is to build hock-huber into my existing 'mathy' approach. 
This will allow me to control the role of luck, to compare apples to apples. It will even let me bring in skill, to fully explore the space. 

I'll also need to come up with the naive winner effects: a n-round boost to estimate, or simple jump in estimate (either additive, or a scale factor)
Once that's done, I can make some contrasting predictions on behavior. 

Keep in mind that Bayes should be the ideal solution to this problem, so the question is whether it is feasible, and whether it's 
actually that much better than a very simple heuristic. 

The other thing to think about is the likelihood function, since that could also be explored. 

