### Add from top:

April 9: 
Ok, I just needed some scratch paper: 
What is going on with the bottom right corner? It corresponds to 
s=16:19, which because it's flipped is probably 2:5, or something like that
So I guess [-0.9:-0.75] ? 
Ternary plots this alongside 
0:2 for luck (so like -1:-0.9) 
0:3 for effort (so like -1:-0.85)

so what's going on there? 

It's obviously a pretty weird space, there's almost 0 luck, so you're 
likelihood function is going to be essentially a line, size and effort have extreme gain as well, so your relative wager is going to be extreme (s shouldn't matter though, since they are the same size
In some cases, your likelihood says that that outcome was impossible, what happens then? nan apparently. Or something very negative. Both are bad

Lots of nans happening when all 3 values are low
lots of min values happening when effort is 1.0 (where effort is just chance)

Jan 27: 
I stopped doing these notes for a while, since I set up a google slides that was easier to work with. 
Remember to copy all that in here for posterity eventually

22 July 5:
Next stop, the duration of the effect. 
I can't actually do duration per se, but I can do how long it lasts given more fights. 

Initially, it seems like the estimate boost lasts essentially forever, but there's relatively little effect on success
Presumably, this is because the increase in their estimate isn't enough to win against a larger fish. 
I could test this as a direct assay against a fish their size after increasingly long time. 

22 June 30:
Ok, after a lot of thought and exploration, I think I've figured it out. 
There are lots of things happening here, 
-the winner effect is more informative, i.e. it produces a more confident, narrower peak
-The loser effect, as an abolute shift, is slightly larger, meaning their esetimate is slightly lower, 
this might be related to the above narrowness, in that losers have more weight in the low distribution. 
-There's also this idea I thought about yesterday, 
where the likelihood function is skewed towards lower values, because it's proportional. 

However, the biggest thing has to do with how effort translates to success, and it's pretty straight forward:
Ignore all the above intricacies, and assume that it's an equal shift, up and down. 
Now the winner has 75% confidence that they're bigger, and the loser only has 25% confidence. 
An accurate, size matched fish is going to have 50% confidence. 

So loser invests 25% vs 50% = 1:2
Winner invests 75% vs 50% = 2:3

So the relative difference is larger for the loser. 

The math of relative differences strikes again. Note that this isn't exactly unique to bayesian updating, this could happen 
with the static shifts too. 


22 June 29:
I started with the winner vs loser effect. 
Interestingly, bayes predicts that the loser effect will be stronger than the winner effect, by virtue of the likelihood func. 
Conceptually, fighting is probalby determined by % difference, not absolute difference, 
but it's obvious that there are going to be more people 50% your size than 200% your size. Because of this, 
the likelihood function (which is calculated in % space but plotted in absolute space) flattens out as you get bigger

Mathematically, we can describe this as the mean of the likelihood functions for winning and losing. 
22 June 28:
Took a little reading break, and then went on vacation. Now I have a pretty clear goal: 
1. Does babyesian updating describe the winner effect in a way that is consistent with what is known? 
2. How does Bayesian update contrast with alternative models? 

So next steps are to compile a list of things I know about the winner effect, and see if/when my model replicates that, 
and then identify places where it's going to break with non-bayesian updating. 

The latter point, is pretty straightforward. The defining feature of bayesian updating, in comparison to a point model, 
is that your estimate can be the same, yet your response to the behavior will be very different, because of the relative confidence. 
How could we tell the difference between this and something else? What would something else even be? 

For now, I can just focus on existing models I guess. 

22 June 13:
On reflection, Bayesian Updating is still a single step process, there's no real history. It's just the dimensionality of the state that changes. 
So in the game theoretic approach, where there are 6 variables, those 6 variables are the memory, and it's hard to say whether that is 
more or less history dependent than bayesian updating or a markov process. What is unique about bayesian updating is that we get a distribution, 
rather than a single point, and so we have some predictions where behavior might be the same, but the reaction to a fight might be different, 
(for example if two fish have the same estimate, but different confidences, the one with lower confidence will shift his estimate more strongly). 


22 June 7:
Running with prior=True causes their estimates to increase constantly. Not sure why. 
Also, it's going really slow at first, because of how intensive it is to generate the likelihood function for each fish. Let's fix that too. 

Easiest way to initialize is to make a pilot fish to build the likelihood, and then feed that likelihood into everyone else. This makes it much faster. For this to work, each fish needs to have an idea of the average sized fish built into their likelihood function. 

This makes me think I could speed things up a ton if I just pre-built the likelihood functions as a dict for OA as well. There's really only as many likelihood functions as their are fish, so it shouldn't be too bad. Not sure where that should live though...
I put it in tank, you just have to initialize it after you build all the fish. In theory, I could have one for all possible fish sizes, but since it's continuous, I'd really need a good analytical solution. for now this is quite good. 

The initialized likelihood is 25x faster, so that's not too shabby. 

Ok, no more optimization, time to do science.

I started in on some big comparisons between a bayes framework and a fixed shift to your estimate. 
I'll need to do the same thing with a decayed shift, and all the other ones, and then parameterize that so that I can run simulations across the entire parameter space for both self assessment and mutual assessment (and maybe opponent assessment?)
see google doc for next tests to do

By the way, it wouldn't actually be that hard to shift the prior (for growth, for example). It's just multiplying by a different likelihood function, basically a monotonic increasing function: As you get older, being bigger becomes more likely. Similarly, decaying a prior would just be a flat likelihood function. This, and fish personality, and probably the most interesting next steps, imo

22 June 6:
I updated fish so that they could start with a correct prior. 
This is sort of the default assumption, so let's see how the winner effect plays
when their estimate is fixed, correct, and gets boosted by win/lose.


22 June 3b:
Spent a long time working on my likelihood function for naive stuff. But things are working again. 
I think the big insight here is that without an accurate self assessment, you're going to be 
writing checks you can't cash, which, depending on how risky that is, could be bad. 

So Measuring fight level in excess of fish size might be a nice one, since that will happen a lot with
all the naive metrics, but doesn't happen much with bayes self assessment. Obviously, when you bring
opponent assessment into the mix, you can be very precise about the amount of effort you put in, 
but maybe self assessment is enough. 

22 June 3:
It occurred to me that maybe the initial prior is good enough that with opponent assessment, 
you will get consistent, accurate hierarchies regardless of the winner effect. 
To test this, I made a 'no_update' function, which just keeps the prior constant. 
Results:
accuracy drops to .82, which is low, but not as low as you might expect...Here the fight's are 
largely determined by size. Obviously if that changes, accuracy will drop as well. 

Frustratingly, bayes still doesn't perform better with pure self assessment. What happens if we make 
effort more influential? The point here also is that I'm measuring the accuracy of fights, not the 
accuracy of assessments. It may be that every fish thinks they're the biggest, even if they lose. 
While this WE is "accurate" in the sense that you don't get wrong hierarchies, there's a lot of wasted 
effort. And if there is a cost to effort, it's going to be bad. So I should be measuring that also...

The fact that bayes' isn't doing anything for me though makes me think there is something wrong...
there were some more bugs, and I started measuring effort and number of upsets, which are a nicer metric. 
Accuracy is okay...but on average, even when things are very bad, if size factors in, the hierarchy will
end up being roughly accurate. You need pretty strong winner effects, driven by initially wild estimates, 
with a lack of good opponent assessment (I should figure out those exact conditions where you get 
confident, stable, incorrect hierarchies, which are, imho, the coolest.) 

In any case, bayes opponent assessment looks like what I would expect, but bayes self assessment 
is pretty poor. They're just not figuring things out very well. I suspect my likelihood function is off. 
Narrator: Likelihood function was indeed off.

22 June 2:
Found a big bug in how effort was working, so everything will need to be redone. I expect this will happen a couple more times...
Still, no harm done yet. 
-I have this method for measuring 'estimate', but effort is nicer to measure. I wrote some code for that. 
 This however brings up the fact that how effort is decided is such a huge part of this. 

It's not just OA vs SA, how much effort should you give? Should you give max effort if you're guaranteed a win? 
Or should you give the minimum effort it requires to win? 
Seeing the biggest fish always giving max effort really raises some question as to the logic of that strategy
I guess I should think of this more as the max bid, but you only have to give as much as the opponent did.

With the effort fixed, I check accuracy. In order, most to least accurate: 
Decay, Fixed, Bayes, (all essentially equal) and then 10% back, Hock-Huber (which is basically ELO)
...It's a bit of a bummer that the stupid approaches are equally (if not more) accurate than bayesian updating. I'll need to think about that a bit. 

22 June 1: 
I think everything works now...some tests would probably help, but I'm going to set up the code to run some tests.
Hopefully any bugs will pop up as I go, if not I'll do some careful testing afterwards. 
Still need to add a little more though: 
-Calculating the longevity of the winner effect (the winner effect strength function is there, just iterate it)
-I feel like stability is still a little sus, but I might not use it that much anyway
Each test might as well be a custom script from here on out. 

I made a little demo of duration and strength being a function of information. I'll need to actually 
write a way to quantify that, so that I can then compare it to other winner-effect models, but we're getting there. 

22 May 31:
I've started working on the more naive winner effect approaches:
- A clipped, decaying boost. Several parameters here, but for now, after fight +/- .1 to effort (clipped to 0,1)
- Non decaying boost (+/- .1 to effort, clipped to (0,1), doesn't decay), just set self.decay = 1
I've also added a tank.function to calculate the strenght of the winner effect for each fish.

I need to clean up the update_prior methods, right now I use one for opponent assessment, 
and a differently structured one for self assessment. Would be much cleaner if they were consistent

22 May 27b:
So, things got too messy with the below approach, so to simplify self assessment, I assumed
that you only know your own effort, and you assume every fish you meet is average size and effort.
This actually seems to work pretty well. With pure SA, you get nice, correct hierarchies (assuming size 
matters somewhat). The function is set up so that you *could* use additional info if you wanted to, 
although I haven't tested that yet. 

22 May 27:
I got back into the likelihood function, and boy is that a mess. 
So it's probably not very reasonable to say that fish know exactly the size of their opponent, especially
if we're doing a Self Assessment model. So in that case, what do fish know? 
    My assumptions are that they know Own Effort, and the Opponent Wager. 
So that's pretty good, and then you can just infer your size based on those two things, EXCEPT!! The 
size variable that goes into determining the fight is relative size. So in order to do a likelihood function
of relative size, you need to know something about the size of the opponent (or the effort). 

I think the most reasonable assumption to make there is that in pure self assessment, you assume average
effort by the other fish. So I set effort to .5, and then infer the size of the opponent from that and then 
calculate a likelihood function for relative size in that way. This is basically a transformation of my size
to relative size for that fight. Obviously, if you know the opponent's size, this is much easier (and I should
probably update that as well eventually...). 

So in summary, under SA, fish base their likelihood functions from their known effort, the assumed effort of the opponent, and the experienced wager from the opponent.

Even this is a little weird, like you only really know that the opponent wagered as much as or more than you...
but this feels fine for now. Definitely something to think about. 

I think in the long run, we'll have these general predictions, and then we'll see what it takes to fit the data, and then we'll ask if such a model is biologically plausible. 

22 May 26:
Did a lot of work to get Hock-Huber approach working. I made a slight modification to the way they calculate probability so that it matches
the more empirically driven idea of the outcome being decided by the percent difference, with marginal effort having a bigger impact when 
you're close to the opponent's investment. 

I also brought it into my wager function, so that I could play with the amount of luck involved, as well as add a size component (by default, 
Hock assumed l=0.5, which is a linear relationship between relative effort and probability of winning)

Surprisingly, although the stability of the hock estimate swings widely, as I would have guessed, the stability of fights is fairly comparable. 
The caveat here is I still don't entirely trust my stability metric...so I should check on that next. 
On top of that, I should probably calculate the strength of the winner effect: i.e. the increased probability of winning following a win. 


22 May 25:
Ran all the simulations, things are looking quite good. Ended with running Hock approach
One thing to keep in mind here is that "stability" measures fight performance, while "estimate" measures internal state. 
So when I'm plotting the est_history, I'm plotting their internal states, while stability is based on their wins/losses. 

In any case, it does seem like Hock is a lot less stable, but that could be in part just because of the inherent luck in the approach. 
The next thing to do in order for a fair comparison is to build hock-huber into my existing 'mathy' approach. 
This will allow me to control the role of luck, to compare apples to apples. It will even let me bring in skill, to fully explore the space. 

I'll also need to come up with the naive winner effects: a n-round boost to estimate, or simple jump in estimate (either additive, or a scale factor)
Once that's done, I can make some contrasting predictions on behavior. 

Keep in mind that Bayes should be the ideal solution to this problem, so the question is whether it is feasible, and whether it's 
actually that much better than a very simple heuristic. 

The other thing to think about is the likelihood function, since that could also be explored. 

